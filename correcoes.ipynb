{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requesting notifica where nt.classificacao_final = 2 AND nt.evolucao = 1\n",
      "Success code 202\n",
      "Saving in input\\queries\\recuperados.csv\n",
      "Deu ruim\n",
      "Requesting notifica where nt.classificacao_final = 2 AND nt.evolucao = 1\n",
      "Deu ruim\n",
      "Requesting notifica where nt.classificacao_final = 2 AND nt.evolucao = 1\n",
      "Deu ruim\n",
      "Requesting notifica where nt.classificacao_final = 2 AND nt.evolucao = 1\n",
      "Success code 202\n",
      "Saving in input\\queries\\recuperados.csv\n",
      "Download finish, time elapsed: 0:00:35.594845\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sys import exit\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import dirname, join, isfile, isdir\n",
    "from os import makedirs\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "from bulletin.data.notifica import Notifica\n",
    "from bulletin.data.casos_confirmados import CasosConfirmados\n",
    "from bulletin.commom.utils import Timer, get_better_notifica\n",
    "from bulletin.commom.static import meses\n",
    "from bulletin.metabase.request import download_metabase\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "output = join(\"output\",\"correcoes\",\"notifica\")\n",
    "\n",
    "if not isdir(output):\n",
    "    makedirs(output)\n",
    "\n",
    "timerAll = Timer()\n",
    "timer = Timer()\n",
    "\n",
    "tentar = True\n",
    "while tentar:\n",
    "    try:\n",
    "        download_metabase('recuperados.csv',where='nt.classificacao_final = 2 AND nt.evolucao = 1')\n",
    "        tentar = False\n",
    "    except Error:\n",
    "        print(f'Deu ruim n\\{Error}')\n",
    "        sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "notifica = Notifica()\n",
    "if input(\"Enter para continuar, S para baixar e ler notifica\") == 'S':\n",
    "    notifica.download_todas_notificacoes()\n",
    "    notifica.read_todas_notificacoes()\n",
    "    notifica.save()\n",
    "else:\n",
    "    notifica.load()\n",
    "\n",
    "print(notifica.shape())\n",
    "\n",
    "casos_confirmados = CasosConfirmados()\n",
    "if input(\"Enter para continuar, S para ler casos confirmados\") == 'S':\n",
    "    casos_confirmados.read()\n",
    "    casos_confirmados.save()\n",
    "else:\n",
    "    casos_confirmados.load()\n",
    "\n",
    "print(casos_confirmados.shape())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casosn = notifica.get_casos()\n",
    "casosn['duplicado'] = 0\n",
    "casosn['manter'] = 0\n",
    "casosc = casos_confirmados.get_casos()\n",
    "casosn.groupby('classificacao_final')[['id']].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_duplicados = casosn.loc[(casosn['cpf'].notnull()) & (casosn.duplicated('cpf',keep=False))].sort_values('cpf')\n",
    "casosn.loc[casos_duplicados.index, 'duplicado'] = 1 \n",
    "\n",
    "print(f\"Total de pacientes com mais de uma ocorrencia: {len(casos_duplicados)}\")\n",
    "print(f\"{len(casos_duplicados.groupby('cpf'))} pacientes pelo CPF que estavam com mais de uma ocorrencia\")\n",
    "\n",
    "all_casos_duplicados = set()\n",
    "all_duplicados_mantidos = set()\n",
    "manter = []\n",
    "\n",
    "for hash, group in casos_duplicados.groupby('cpf'):\n",
    "    idx = get_better_notifica(group)\n",
    "    #check reinfection\n",
    "    manter.append(idx)\n",
    "\n",
    "\n",
    "casosn.loc[manter, 'manter'] = 1 \n",
    "\n",
    "casos_duplicados = set(casos_duplicados.index.tolist())\n",
    "casos_duplicados = casos_duplicados - set(manter)\n",
    "\n",
    "all_duplicados_mantidos |= set(manter)\n",
    "all_casos_duplicados |= casos_duplicados\n",
    "casosn.loc[casos_duplicados].to_csv(join(output,'pacientes_duplicados_cpf.csv'), index=False)\n",
    "\n",
    "casosn.loc[casos_duplicados].groupby('classificacao_final')[['id']].count()\n",
    "# casosn = casosn.drop(index=casos_duplicados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_duplicados = casosn.loc[(casosn['nome_mae'].notnull()) & (casosn.duplicated('hash_mae',keep=False))].sort_values('nome_mae')\n",
    "casosn.loc[casos_duplicados.index, 'duplicado'] = 1 \n",
    "\n",
    "print(f\"Total de pacientes com mais de uma ocorrencia pelo nome+nome_mae: {len(casos_duplicados)}\")\n",
    "print(f\"{len(casos_duplicados.groupby('nome_mae'))} pacientes pelo nome+nome_mae que estavam com mais de uma ocorrencia\")\n",
    "\n",
    "manter = []\n",
    "for hash, group in casos_duplicados.groupby('nome_mae'):\n",
    "    idx = get_better_notifica(group)\n",
    "    manter.append(idx)\n",
    "\n",
    "casosn.loc[manter, 'manter'] = 1 \n",
    "\n",
    "casos_duplicados = set(casos_duplicados.index.tolist())\n",
    "casos_duplicados = casos_duplicados - set(manter)\n",
    "\n",
    "all_duplicados_mantidos |= set(manter)\n",
    "all_casos_duplicados |= casos_duplicados\n",
    "casosn.loc[casos_duplicados].to_csv(join(output,'pacientes_duplicados_nome_mae.csv'), index=False)\n",
    "\n",
    "casosn.loc[casos_duplicados].groupby('classificacao_final')[['id']].count()\n",
    "# casosn = casosn.drop(index=casos_duplicados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_duplicados = casosn.loc[(casosn['hash_nasc'].notnull()) & (casosn.duplicated('hash_nasc',keep=False))].sort_values('data_nascimento')\n",
    "casosn.loc[casos_duplicados.index, 'duplicado'] = 1 \n",
    "\n",
    "print(f\"Total de pacientes com mais de uma ocorrencia pelo data_nascimento: {len(casos_duplicados)}\")\n",
    "print(f\"{len(casos_duplicados.groupby('hash_nasc'))} pacientes pelo data_nascimento que estavam com mais de uma ocorrencia\")\n",
    "\n",
    "manter = []\n",
    "for hash, group in casos_duplicados.groupby('hash_nasc'):\n",
    "    idx = get_better_notifica(group)\n",
    "    manter.append(idx)\n",
    "\n",
    "casosn.loc[manter, 'manter'] = 1 \n",
    "\n",
    "casos_duplicados = set(casos_duplicados.index.tolist())\n",
    "casos_duplicados = casos_duplicados - set(manter)\n",
    "\n",
    "all_duplicados_mantidos |= set(manter)\n",
    "all_casos_duplicados |= casos_duplicados\n",
    "casosn.loc[casos_duplicados].to_csv(join(output,'pacientes_duplicados_data_nascimento.csv'), index=False)\n",
    "\n",
    "casosn.loc[casos_duplicados].groupby('classificacao_final')[['id']].count()\n",
    "# casosn = casosn.drop(index=casos_duplicados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_duplicados = casosn.loc[(casosn['hash_resid'].notnull()) & (casosn.duplicated('hash_resid',keep=False))].sort_values('paciente')\n",
    "casosn.loc[casos_duplicados.index, 'duplicado'] = 1 \n",
    "\n",
    "print(f\"Total de pacientes com mais de uma ocorrencia pelo hash_resid: {len(casos_duplicados)}\")\n",
    "print(f\"{len(casos_duplicados.groupby('hash_resid'))} pacientes pelo hash_resid que estavam com mais de uma ocorrencia\")\n",
    "\n",
    "manter = []\n",
    "for hash, group in casos_duplicados.groupby('hash_resid'):\n",
    "    idx = get_better_notifica(group)\n",
    "    manter.append(idx)\n",
    "\n",
    "casosn.loc[manter, 'manter'] = 1 \n",
    "\n",
    "casos_duplicados = set(casos_duplicados.index.tolist())\n",
    "casos_duplicados = casos_duplicados - set(manter)\n",
    "\n",
    "all_duplicados_mantidos |= set(manter)\n",
    "all_casos_duplicados |= casos_duplicados\n",
    "casosn.loc[casos_duplicados].to_csv(join(output,'pacientes_duplicados_nome_idade_mun_resid.csv'), index=False)\n",
    "\n",
    "casosn.loc[casos_duplicados].groupby('classificacao_final')[['id']].count()\n",
    "# casosn = casosn.drop(index=casos_duplicados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_duplicados = casosn.loc[(casosn['hash_atend'].notnull()) & (casosn.duplicated('hash_atend',keep=False))].sort_values('paciente')\n",
    "casosn.loc[casos_duplicados.index, 'duplicado'] = 1 \n",
    "\n",
    "print(f\"Total de pacientes com mais de uma ocorrencia pelo hash_atend: {len(casos_duplicados)}\")\n",
    "print(f\"{len(casos_duplicados.groupby('hash_atend'))} pacientes pelo hash_atend que estavam com mais de uma ocorrencia\")\n",
    "\n",
    "manter = []\n",
    "for hash, group in casos_duplicados.groupby('hash_atend'):\n",
    "    idx = get_better_notifica(group)\n",
    "    manter.append(idx)\n",
    "\n",
    "casosn.loc[manter, 'manter'] = 1 \n",
    "\n",
    "casos_duplicados = set(casos_duplicados.index.tolist())\n",
    "casos_duplicados = casos_duplicados - set(manter)\n",
    "\n",
    "all_duplicados_mantidos |= set(manter)\n",
    "all_casos_duplicados |= casos_duplicados\n",
    "casosn.loc[casos_duplicados].to_csv(join(output,'pacientes_duplicados_nome_idade_mun_atend.csv'), index=False)\n",
    "\n",
    "casosn.loc[casos_duplicados].groupby('classificacao_final')[['id']].count()\n",
    "# casosn = casosn.drop(index=casos_duplicados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_duplicados = casosn.loc[(casosn['hash_diag'].notnull()) & (casosn.duplicated('hash_diag',keep=False))].sort_values('paciente')\n",
    "casosn.loc[casos_duplicados.index, 'duplicado'] = 1 \n",
    "\n",
    "print(f\"Total de pacientes com mais de uma ocorrencia pelo hash_diag: {len(casos_duplicados)}\")\n",
    "print(f\"{len(casos_duplicados.groupby('hash_diag'))} pacientes pelo hash_diag que estavam com mais de uma ocorrencia\")\n",
    "\n",
    "manter = []\n",
    "for hash, group in casos_duplicados.groupby('hash_diag'):\n",
    "    idx = get_better_notifica(group)\n",
    "    manter.append(idx)\n",
    "\n",
    "casosn.loc[manter, 'manter'] = 1 \n",
    "\n",
    "casos_duplicados = set(casos_duplicados.index.tolist())\n",
    "casos_duplicados = casos_duplicados - set(manter)\n",
    "\n",
    "all_duplicados_mantidos |= set(manter)\n",
    "all_casos_duplicados |= casos_duplicados\n",
    "casosn.loc[casos_duplicados].to_csv(join(output,'pacientes_duplicados_hash_diag.csv'), index=False)\n",
    "\n",
    "casosn.loc[casos_duplicados].groupby('classificacao_final')[['id']].count()\n",
    "# casosn = casosn.drop(index=casos_duplicados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notifica.save(casosn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}